from doctest import Example
from openai import OpenAI
from dotenv import load_dotenv
import os
import json
import function_tools
import requests
import gradio as gr
import random
import function_tools  # æ·»åŠ å¯¼å…¥è¯­å¥

def weather_search():
    logging.basicConfig(level=logging.INFO)
    log = logging.getLogger(__name__)
    log.info("weather_search")
    # åŸºæœ¬å‚æ•°é…ç½®
    apiUrl = 'http://apis.juhe.cn/simpleWeather/query'  # æ¥å£è¯·æ±‚URL
    apiKey = '18108b0f28a5a63960cd38108aab778f'  # åœ¨ä¸ªäººä¸­å¿ƒ->æˆ‘çš„æ•°æ®,æ¥å£åç§°ä¸Šæ–¹æŸ¥çœ‹

    # æ¥å£è¯·æ±‚å…¥å‚é…ç½®
    requestParams = {
        'key':apiKey,
        'city': 'è‹å·',
    }

    # å‘èµ·æ¥å£ç½‘ç»œè¯·æ±‚
    response = requests.get(apiUrl, params=requestParams)

    # è§£æå“åº”ç»“æœ
    if response.status_code == 200:
        responseResult = response.json()
        return f'{responseResult["result"]}'
    else:
        # ç½‘ç»œå¼‚å¸¸ç­‰å› ç´ ï¼Œè§£æç»“æœå¼‚å¸¸ã€‚å¯ä¾æ®ä¸šåŠ¡é€»è¾‘è‡ªè¡Œå¤„ç†ã€‚
        print('è¯·æ±‚å¼‚å¸¸')

def process_query(input_text, platform, model, temperature, max_tokens):
    try:
        load_dotenv()
        client = OpenAI(
            api_key=os.getenv("ZHIPU_API_KEY"),
            base_url=os.getenv("ZHIPU_API_BASE_URL")
        )
        
        tools = [function_tools.WEATHER_SEARCH]
        messages = [
            {'role':'system','content':'ç›´æ¥è°ƒç”¨å·¥å…·å›ç­”ç”¨æˆ·é—®é¢˜'},
            {'role':'user','content':input_text}
        ]

        response = client.chat.completions.create(
            model=model,
            tools=tools,
            messages=messages
        )

        if response.choices[0].message.tool_calls:
            for tool_call in response.choices[0].message.tool_calls:
                args = json.loads(tool_call.function.arguments)
                function_name = tool_call.function.name
                invoke_fun = getattr(function_tools, function_name)
                result = invoke_fun(**args)
                messages.append({
                    'role': 'tool',
                    'content': json.dumps(result, ensure_ascii=False),
                    'tool_call_id': tool_call.id
                })
            
            response = client.chat.completions.create(
                model=model,
                messages=messages
            )

        return response.choices[0].message.content, ""

    except Exception as e:
        return response.choices[0].message.content, 
# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="å¤©æ°”æŸ¥è¯¢åŠ©æ‰‹", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸŒ¤ï¸ å®æˆ˜ï¼šå¤©æ°”æŸ¥è¯¢åŠ©æ‰‹ï¼ˆFunction Callingï¼‰")
    
    with gr.Row():
        with gr.Column(scale=1):
            # è¾“å…¥éƒ¨åˆ†
            input_text = gr.Textbox(label="è¯·è¾“å…¥", placeholder="ä¾‹å¦‚ï¼šä»Šå¤©åŒ—äº¬å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
               # æ¨¡å‹å›å¤
            model_response = gr.Textbox(
                label="æ¨¡å‹å›å¤", 
                value="åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ˜¯é˜´å¤©ï¼Œæ°”æ¸©ä¸º17æ‘„æ°åº¦ï¼Œæ¹¿åº¦ä¸º82%ï¼Œç©ºæ°”è´¨é‡æŒ‡æ•°ä¸º57ã€‚",
                lines=4,
                interactive=False
            )
            
            submit_btn = gr.Button("æäº¤", variant="primary")
        
        with gr.Column(scale=2):
         
                # æ¨¡å‹å¹³å°é€‰æ‹©
            platform = gr.Radio(
                choices=["openai", "ZhipuAI", "Bailian"],
                label="å¤§æ¨¡å‹å¹³å°",
                value="openai"
            )
            
            # æ¨¡å‹é€‰æ‹©
            model = gr.Dropdown(
                choices=["gpt-3.5-turbo", "gpt-4", "glm-4", "bailian-1.0"],
                label="æ¨¡å‹åç§°",
                value="gpt-3.5-turbo"
            )
            
            # å‚æ•°è®¾ç½®
            temperature = gr.Slider(
                minimum=0.1, maximum=1.0, step=0.1,
                value=0.8, label="temperature"
            )
            
            max_tokens = gr.Slider(
                minimum=128, maximum=1024, step=64,
                value=512, label="max_tokens"
            )
    # æäº¤æŒ‰é’®äº‹ä»¶
    submit_btn.click(
        fn=process_query,
        inputs=[input_text, platform, model, temperature, max_tokens],
        outputs=[model_response]
    )

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    demo.launch()